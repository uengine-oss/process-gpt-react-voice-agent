import asyncio
import json
import websockets

from contextlib import asynccontextmanager
from typing import AsyncGenerator, AsyncIterator, Any, Callable, Coroutine
from langchain_openai_voice.utils import amerge

from langchain_core.tools import BaseTool
from langchain_core._api import beta
from langchain_core.utils import secret_from_env

from pydantic import BaseModel, Field, SecretStr, PrivateAttr

DEFAULT_MODEL = "gpt-4o-realtime-preview-2024-10-01"
DEFAULT_URL = "wss://api.openai.com/v1/realtime"

EVENTS_TO_IGNORE = {
    "response.function_call_arguments.delta",
    "rate_limits.updated",
    "response.created",
    "response.content_part.added",
    "response.content_part.done",
    "conversation.item.created",
    "session.created",
    "session.updated",
    "response.output_item.done",
}


@asynccontextmanager
async def connect(*, api_key: str, model: str, url: str, max_retries: int = 3) -> AsyncGenerator[
    tuple[
        Callable[[dict[str, Any] | str], Coroutine[Any, Any, None]],
        AsyncIterator[dict[str, Any]],
    ],
    None,
]:
    """
    async with connect(model="gpt-4o-realtime-preview-2024-10-01") as websocket:
        await websocket.send("Hello, world!")
        async for message in websocket:
            print(message)
    """

    headers = {
        "Authorization": f"Bearer {api_key}",
        "OpenAI-Beta": "realtime=v1",
    }

    url = url or DEFAULT_URL
    url += f"?model={model}"

    websocket = None
    last_error = None
    
    for attempt in range(max_retries):
        try:
            websocket = await websockets.connect(
                url,
                additional_headers=headers,  # websockets 12.x
                open_timeout=60,
                close_timeout=60,
                ping_interval=20,    # 20ì´ˆë§ˆë‹¤ ping (ì—°ê²° ìƒíƒœ ëª¨ë‹ˆí„°ë§)
                ping_timeout=10,     # 10ì´ˆ ë‚´ pong ì‘ë‹µ ëŒ€ê¸°
                max_size=2**20     # ìµœëŒ€ ë©”ì‹œì§€ í¬ê¸°
            )
            break
        except Exception as e:
            last_error = e
            if attempt < max_retries - 1:
                await asyncio.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
            else:
                print(f"WebSocket ì—°ê²° ì‹¤íŒ¨: {e}")
                raise last_error

    try:

        async def send_event(event: dict[str, Any] | str) -> None:
            try:
                formatted_event = json.dumps(event) if isinstance(event, dict) else event
                await websocket.send(formatted_event)
            except Exception as e:
                raise

        async def event_stream() -> AsyncIterator[dict[str, Any]]:
            async for raw_event in websocket:
                yield json.loads(raw_event)

        stream: AsyncIterator[dict[str, Any]] = event_stream()

        yield send_event, stream
    finally:
        await websocket.close()


class VoiceToolExecutor(BaseModel):
    """
    Can accept function calls and emits function call outputs to a stream.
    """

    tools_by_name: dict[str, BaseTool]
    _trigger_future: asyncio.Future = PrivateAttr(default_factory=asyncio.Future)
    _lock: asyncio.Lock = PrivateAttr(default_factory=asyncio.Lock)

    async def _trigger_func(self) -> dict:  # returns a tool call
        return await self._trigger_future

    async def add_tool_call(self, tool_call: dict) -> None:
        # lock to avoid simultaneous tool calls racing and missing
        # _trigger_future being
        async with self._lock:
            if self._trigger_future.done():
                # TODO: handle simultaneous tool calls better
                raise ValueError("Tool call adding already in progress")

            self._trigger_future.set_result(tool_call)

    async def _create_tool_call_task(self, tool_call: dict) -> asyncio.Task[dict]:
        tool = self.tools_by_name.get(tool_call["name"])
        if tool is None:
            # immediately yield error, do not add task
            raise ValueError(
                f"tool {tool_call['name']} not found. "
                f"Must be one of {list(self.tools_by_name.keys())}"
            )

        # try to parse args
        try:
            args = json.loads(tool_call["arguments"])
        except json.JSONDecodeError:
            raise ValueError(
                f"failed to parse arguments `{tool_call['arguments']}`. Must be valid JSON."
            )

        async def run_tool() -> dict:
            try:
                result = await tool.ainvoke(args)
                try:
                    result_str = json.dumps(result)
                except TypeError:
                    # not json serializable, use str
                    result_str = str(result)
                return {
                    "type": "conversation.item.create",
                    "item": {
                        "id": tool_call["call_id"],
                        "call_id": tool_call["call_id"],
                        "type": "function_call_output",
                        "output": result_str,
                    },
                }
            except Exception as e:
                # í´ë°±: ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ì‚¬ìš©ìì—ê²Œ ë°˜í™˜
                return {
                    "type": "conversation.item.create",
                    "item": {
                        "id": tool_call["call_id"],
                        "call_id": tool_call["call_id"],
                        "type": "function_call_output",
                        "output": f"ë„êµ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                    },
                }

        task = asyncio.create_task(run_tool())
        return task

    async def output_iterator(self) -> AsyncIterator[dict]:  # yield events
        trigger_task = asyncio.create_task(self._trigger_func())
        tasks = set([trigger_task])
        while True:
            done, _ = await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)
            for task in done:
                tasks.remove(task)
                if task == trigger_task:
                    async with self._lock:
                        self._trigger_future = asyncio.Future()
                    trigger_task = asyncio.create_task(self._trigger_func())
                    tasks.add(trigger_task)
                    tool_call = task.result()
                    try:
                        new_task = await self._create_tool_call_task(tool_call)
                        tasks.add(new_task)
                    except ValueError as e:
                        yield {
                            "type": "conversation.item.create",
                            "item": {
                                "id": tool_call["call_id"],
                                "call_id": tool_call["call_id"],
                                "type": "function_call_output",
                                "output": (f"Error: {str(e)}"),
                            },
                        }
                else:
                    yield task.result()



@beta()
class OpenAIVoiceReactAgent(BaseModel):
    model: str
    openai_api_key: str
    instructions: str | None = None
    tools: list[BaseTool] | None = None
    url: str = Field(default=DEFAULT_URL)
    user_info: dict[str, Any] | None = None
    conversation_history: list[dict[str, Any]] | None = None
    silence_duration_ms: int = Field(default=1200)  # 1.2ì´ˆ ì¹¨ë¬µ ê°ì§€ (ë¹ ë¥¸ ì‘ë‹µ)
    
    # í„´ ê´€ë¦¬ ìƒíƒœ ë³€ìˆ˜
    _is_ai_responding: bool = PrivateAttr(default=False)
    _is_user_speaking: bool = PrivateAttr(default=False)
    _has_audio_response: bool = PrivateAttr(default=False)

    async def aconnect(
        self,
        input_stream: AsyncIterator[str],
        send_output_chunk: Callable[[str], Coroutine[Any, Any, None]],
    ) -> None:
        """
        Connect to the OpenAI API and send and receive messages.

        input_stream: AsyncIterator[str]
            Stream of input events to send to the model. Usually transports input_audio_buffer.append events from the microphone.
        output: Callable[[str], None]
            Callback to receive output events from the model. Usually sends response.audio.delta events to the speaker.

        """
        # formatted_tools: list[BaseTool] = [
        #     tool if isinstance(tool, BaseTool) else tool_converter.wr(tool)  # type: ignore
        #     for tool in self.tools or []
        # ]
        tools_by_name = {tool.name: tool for tool in self.tools}
        tool_executor = VoiceToolExecutor(tools_by_name=tools_by_name)

        # user_infoë¥¼ instructionsì— ì¶”ê°€
        instructions = self.instructions or ""
        if self.user_info:
            user_info_str = "\n".join(f"{k}: {v}" for k, v in self.user_info.items())
            instructions += f"\n[User Info]\n{user_info_str}"

        async with connect(
            model=self.model, api_key=self.openai_api_key, url=self.url
        ) as (
            model_send,
            model_receive_stream,
        ):
            # sent tools and instructions with initial chunk
            tool_defs = [
                {
                    "type": "function",
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": {"type": "object", "properties": tool.args},
                }
                for tool in tools_by_name.values()
            ]
            await model_send(
                {
                    "type": "session.update",
                    "session": {
                        "instructions": instructions,
                        "input_audio_transcription": {
                            "model": "whisper-1",
                        },
                        "turn_detection": {
                            "type": "server_vad",
                            "threshold": 0.8,  # ì¡ìŒ í•„í„°ë§ ê°•í™” (0.6 â†’ 0.8)
                            "prefix_padding_ms": 200,
                            "silence_duration_ms": self.silence_duration_ms
                        },
                        "tools": tool_defs,
                    },
                }
            )

            # ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ OpenAI Realtime ì»¨í…ìŠ¤íŠ¸ì— ì£¼ì…í•œë‹¤.
            # conversation.item.create ì´ë²¤íŠ¸ë¡œ ì´ì „ í„´ì„ ì¬í˜„í•´ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìœ ì§€í•œë‹¤.
            for msg in (self.conversation_history or []):
                role = msg.get("role", "")
                content = (msg.get("content") or "").strip()
                if role not in ("user", "assistant") or not content:
                    continue
                content_type = "input_text" if role == "user" else "text"
                try:
                    await model_send({
                        "type": "conversation.item.create",
                        "item": {
                            "type": "message",
                            "role": role,
                            "content": [{"type": content_type, "text": content}],
                        },
                    })
                except Exception as e:
                    print(f"íˆìŠ¤í† ë¦¬ ì£¼ì… ì‹¤íŒ¨ ({role}): {e}")
            if self.conversation_history:
                print(f"âœ… ëŒ€í™” íˆìŠ¤í† ë¦¬ {len(self.conversation_history)}í„´ ì£¼ì… ì™„ë£Œ")

            async for stream_key, data_raw in amerge(
                input_mic=input_stream,
                output_speaker=model_receive_stream,
                tool_outputs=tool_executor.output_iterator(),
            ):
                try:
                    data = (
                        json.loads(data_raw) if isinstance(data_raw, str) else data_raw
                    )
                except json.JSONDecodeError:
                    continue

                if stream_key == "input_mic":
                    # í´ë¼ì´ì–¸íŠ¸ì—ì„œ ë³´ë‚´ëŠ” ì´ë²¤íŠ¸ ì²˜ë¦¬
                    if isinstance(data, dict):
                        event_type = data.get("type")
                        
                        if event_type == "client_audio_playback_complete":
                            print("AI ì‘ë‹µ ì™„ë£Œ")
                            if self._is_ai_responding and self._has_audio_response:
                                self._is_ai_responding = False
                                self._has_audio_response = False
                            # OpenAI APIë¡œ ì „ì†¡í•˜ì§€ ì•ŠìŒ
                            continue
                            
                        # AIê°€ ì‘ë‹µ ì¤‘ì¼ ë•ŒëŠ” ì‚¬ìš©ì ìŒì„± ì´ë²¤íŠ¸ ë¬´ì‹œ (ì˜¤ë””ì˜¤ ë°ì´í„°ëŠ” ê³„ì† ì²˜ë¦¬)
                        if self._is_ai_responding and event_type in ["input_audio_buffer.speech_started", "input_audio_buffer.speech_stopped"]:
                            print(f"AI ì‘ë‹µ ì¤‘ì´ë¯€ë¡œ ìŒì„± ì´ë²¤íŠ¸ ë¬´ì‹œ: {event_type}")
                            continue
                            
                    await model_send(data)
                elif stream_key == "tool_outputs":
                    await model_send(data)
                    # íˆ´ ì‹¤í–‰ í›„ AI ì‘ë‹µ ì‹œì‘ (í„´ ê¸°ë°˜)
                    if not self._is_user_speaking:
                        self._is_ai_responding = True
                        self._has_audio_response = False  # ìƒˆ ì‘ë‹µ ì‹œì‘ ì‹œ ë¦¬ì…‹
                        await model_send({"type": "response.create", "response": {}})
                elif stream_key == "output_speaker":
                    t = data["type"]
                    
                    # OpenAI VAD ì´ë²¤íŠ¸ ì²˜ë¦¬
                    if t == "input_audio_buffer.speech_started":
                        print("ğŸ¤ ì‚¬ìš©ì ìŒì„± ì‹œì‘ ê°ì§€")
                        self._is_user_speaking = True
                        # ì¸í„°ëŸ½íŠ¸: AI ì‘ë‹µ ì¤‘ ì‚¬ìš©ìê°€ ë°œí™”í•˜ë©´ í˜„ì¬ ì‘ë‹µ ì·¨ì†Œ
                        if self._is_ai_responding:
                            print("âš¡ ì¸í„°ëŸ½íŠ¸: AI ì‘ë‹µ ì·¨ì†Œ")
                            self._is_ai_responding = False
                            self._has_audio_response = False
                            try:
                                await model_send({"type": "response.cancel"})
                            except Exception as e:
                                print(f"response.cancel ì „ì†¡ ì‹¤íŒ¨: {e}")
                        # í´ë¼ì´ì–¸íŠ¸ì— ì „ë‹¬ (ì˜¤ë””ì˜¤ ì¦‰ì‹œ ì¤‘ë‹¨ íŠ¸ë¦¬ê±°)
                        try:
                            await send_output_chunk(json.dumps(data))
                        except Exception as e:
                            print(f"speech_started ì „ì†¡ ì‹¤íŒ¨: {e}")
                    elif t == "input_audio_buffer.speech_stopped":
                        print("ğŸ›‘ ì‚¬ìš©ì ìŒì„± ì¢…ë£Œ ê°ì§€")
                        self._is_user_speaking = False
                        # í´ë¼ì´ì–¸íŠ¸ì— ì „ë‹¬
                        try:
                            await send_output_chunk(json.dumps(data))
                        except Exception as e:
                            print(f"speech_stopped ì „ì†¡ ì‹¤íŒ¨: {e}")
                        # ì‚¬ìš©ì ìŒì„± ì™„ë£Œ í›„ AI ì‘ë‹µ íŠ¸ë¦¬ê±° (í„´ ê¸°ë°˜)
                        if not self._is_ai_responding:
                            print("ğŸš€ AI ì‘ë‹µ ì‹œì‘")
                            self._is_ai_responding = True
                            self._has_audio_response = False  # ìƒˆ ì‘ë‹µ ì‹œì‘ ì‹œ ë¦¬ì…‹
                            await model_send({"type": "response.create", "response": {}})
                    elif t == "input_audio_buffer.committed":
                        print("ğŸ“ ì‚¬ìš©ì ì˜¤ë””ì˜¤ ë²„í¼ ì»¤ë°‹ë¨")
                    
                    # AI ì‘ë‹µ ìƒíƒœ ê´€ë¦¬
                    elif t == "response.audio.delta":
                        if not self._is_ai_responding:
                            print("ğŸ¬ AI ì˜¤ë””ì˜¤ ì‘ë‹µ ì‹œì‘")
                        self._is_ai_responding = True
                        self._has_audio_response = True  # ì˜¤ë””ì˜¤ ì‘ë‹µ ê°ì§€
                        try:
                            await send_output_chunk(json.dumps(data))
                        except Exception as e:
                            print(f"ì˜¤ë””ì˜¤ ë¸íƒ€ ì „ì†¡ ì‹¤íŒ¨: {e}")
                            break
                    elif t == "response.function_call_arguments.done":
                        self._is_ai_responding = True
                        await tool_executor.add_tool_call(data)
                    elif t == "response.audio.done":
                        print("ğŸ¯ AI ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ - í”„ë¡ íŠ¸ì—”ë“œë¡œ ì „ì†¡")
                        try:
                            await send_output_chunk(json.dumps(data))  # í”„ë¡ íŠ¸ì—”ë“œë¡œ ì „ì†¡
                        except Exception as e:
                            print(f"ì˜¤ë””ì˜¤ ì™„ë£Œ ì‹ í˜¸ ì „ì†¡ ì‹¤íŒ¨: {e}")
                            # WebSocket ëŠì–´ì§„ ê²½ìš°ì—ë„ ìƒíƒœ ì •ë¦¬
                            if self._is_ai_responding and self._has_audio_response:
                                print("ì—°ê²° ëŠì–´ì§ìœ¼ë¡œ ì¸í•œ ìƒíƒœ ì •ë¦¬")
                                self._is_ai_responding = False
                                self._has_audio_response = False
                    elif t == "response.audio_transcript.delta":
                        try:
                            await send_output_chunk(json.dumps(data))
                        except Exception as e:
                            print(f"íŠ¸ëœìŠ¤í¬ë¦½íŠ¸ ë¸íƒ€ ì „ì†¡ ì‹¤íŒ¨: {e}")
                    elif t == "response.audio_transcript.done":
                        print("model:", data.get("transcript", ""))
                        try:
                            await send_output_chunk(json.dumps(data))
                        except Exception as e:
                            print(f"íŠ¸ëœìŠ¤í¬ë¦½íŠ¸ ì™„ë£Œ ì „ì†¡ ì‹¤íŒ¨: {e}")
                    elif t == "conversation.item.input_audio_transcription.completed":
                        print("user:", data.get("transcript", ""))
                        try:
                            await send_output_chunk(json.dumps(data))
                        except Exception as e:
                            print(f"ì‚¬ìš©ì íŠ¸ëœìŠ¤í¬ë¦½íŠ¸ ì „ì†¡ ì‹¤íŒ¨: {e}")
                    elif t == "response.done":
                        status = (data.get("response") or {}).get("status", "")
                        print(f"ğŸ“ response.done (ìƒíƒœ: {status}, ì˜¤ë””ì˜¤ ì‘ë‹µ: {self._has_audio_response})")
                        if status == "cancelled":
                            # ì¸í„°ëŸ½íŠ¸ë¡œ ì·¨ì†Œëœ ì‘ë‹µ: ìƒíƒœ ë¦¬ì…‹
                            print("ğŸ“ response.done (ì·¨ì†Œë¨) - ìƒíƒœ ë¦¬ì…‹")
                            self._is_ai_responding = False
                            self._has_audio_response = False
                        elif not self._has_audio_response:
                            # í…ìŠ¤íŠ¸ë§Œ ìˆëŠ” ì‘ë‹µì¸ ê²½ìš°ì—ë§Œ ì—¬ê¸°ì„œ ì™„ë£Œ ì²˜ë¦¬
                            print("ğŸ“ í…ìŠ¤íŠ¸ ì „ìš© ì‘ë‹µ ì™„ë£Œ")
                            self._is_ai_responding = False
                        else:
                            # ì˜¤ë””ì˜¤ ì‘ë‹µì´ ìˆëŠ”ë° response.audio.doneì´ ì˜¤ì§€ ì•ŠëŠ” ê²½ìš° ëŒ€ë¹„
                            print("âš ï¸ ì˜¤ë””ì˜¤ ì‘ë‹µ ì™„ë£Œ - response.audio.done ë¯¸ìˆ˜ì‹ ìœ¼ë¡œ ì—¬ê¸°ì„œ ì²˜ë¦¬")
                            # ì—¬ê¸°ì„œëŠ” ìƒíƒœ ë³€ê²½í•˜ì§€ ì•Šê³  í”„ë¡ íŠ¸ì—”ë“œ ì‹ í˜¸ë§Œ ê¸°ë‹¤ë¦¼
                    elif t == "error":
                        error_code = data.get("error", {}).get("code")
                        if error_code != "conversation_already_has_active_response":
                            print("error:", data)
                        self._is_ai_responding = False  # ì˜¤ë¥˜ ì‹œì—ë„ ìƒíƒœ ë¦¬ì…‹
                    elif t in EVENTS_TO_IGNORE:
                        pass
                    else:
                        print(f"ğŸ” ì²˜ë¦¬ë˜ì§€ ì•Šì€ ì´ë²¤íŠ¸: {t}")


__all__ = ["OpenAIVoiceReactAgent"]
